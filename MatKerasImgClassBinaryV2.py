'''
Created on Nov 19, 2017

@author: mat

Model training

Based on:
https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py
https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d
https://keras.io/preprocessing/image/
'''

from __future__ import print_function
from keras.models import load_model
import os
import MatUtils
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint

MatUtils.setProcessLowPriority()

epochs = 50
# init_lr=1e-3
batch_size=40
save_dir = "C:\\temp\\tensorflow\\model\\keras"
model_name = '100-80weights-improvement-02-0.99.hdf5'
modelPath=os.path.join(save_dir, model_name)
TRAINING_PATH='C:\\temp\\tensorflow\\data\\training'
TEST_PATH='C:\\temp\\tensorflow\\data\\test'

ImageColors = 3
ImageWidth = 200
ImageHeight = 112
num_classes = 2

chkpointFile=os.path.join(save_dir, "weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5")
checkpoint = ModelCheckpoint(chkpointFile, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

if os.path.isfile(modelPath):
    print("Will continue training existing model")
    model = load_model(modelPath)
else:
    print("Training new model")
    model = Sequential()
    model.add(Conv2D(16, 3, strides=1, padding='same', input_shape=(ImageHeight, ImageWidth, ImageColors), activation='relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Conv2D(32, 3, strides=1, padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))
#     opt = Adam(lr=init_lr, decay=init_lr/epochs)
    model.compile(loss='categorical_crossentropy',
                  optimizer='Adam',
                  metrics=['accuracy'])

#     model = Sequential()
#     model.add(Conv2D(16, 3, strides=1, input_shape=(ImageHeight, ImageWidth, ImageColors), activation='relu'))
#     model.add(MaxPooling2D(pool_size=2))
#     model.add(Conv2D(8, 3, strides=1, activation='relu'))
#     model.add(MaxPooling2D(pool_size=2))
#     model.add(Flatten())
#     model.add(Dense(512, activation='relu'))
#     model.add(Dense(1))
#     model.add(Activation('sigmoid'))
#     model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# This will do preprocessing and realtime data augmentation:
train_datagen = ImageDataGenerator(
     rescale=1./255)#,
#      featurewise_center=False,  # set input mean to 0 over the dataset
#      samplewise_center=False,  # set each sample mean to 0
#      featurewise_std_normalization=False,  # divide inputs by std of the dataset
#      samplewise_std_normalization=False,  # divide each input by its std
#      zca_whitening=False,  # apply ZCA whitening
#      # randomly rotate images in the range (degrees, 0 to 180)
#      rotation_range=3,
#      # randomly shift images horizontally (fraction of total width)
#      width_shift_range=0.2,
#      # randomly shift images vertically (fraction of total height)
#      height_shift_range=0.2,
#      horizontal_flip=True,  
#      vertical_flip=False,
#      fill_mode='nearest')  

train_generator = train_datagen.flow_from_directory(
    TRAINING_PATH,
    target_size=(ImageHeight,ImageWidth),
    batch_size=batch_size,
    class_mode='categorical')

test_datagen = ImageDataGenerator(rescale=1. / 255)
test_generator = test_datagen.flow_from_directory(
    TEST_PATH,
    target_size=(ImageHeight, ImageWidth),
    batch_size=batch_size,
    class_mode='categorical')
#     # Compute quantities required for feature-wise normalization
#     # (std, mean, and principal components if ZCA whitening is applied).
#     datagen.fit(trainingImages)

nb_train_samples = MatUtils.getImageFileCountFromPath(TRAINING_PATH+'\\cool')
nb_train_samples = nb_train_samples + MatUtils.getImageFileCountFromPath(TRAINING_PATH+'\\notcool')
nb_test_samples = MatUtils.getImageFileCountFromPath(TEST_PATH+'\\cool')
nb_test_samples = nb_test_samples + MatUtils.getImageFileCountFromPath(TEST_PATH+'\\notcool')
print('nb_train_samples: ',nb_train_samples)
print('nb_test_samples: ',nb_test_samples)

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(train_generator,
                    steps_per_epoch=nb_train_samples // batch_size,
                    callbacks=callbacks_list,
                    epochs=epochs,
                    validation_data=test_generator,
                    validation_steps=nb_test_samples // batch_size,
                    verbose=1)
#                         workers=4)

# model.save_weights('C:\\temp\\tensorflow\\model\\keras\\first_try.h5')
# Save model and weights
# if not os.path.isdir(save_dir):
#     os.makedirs(save_dir)
# model_path = modelPath
# model.save(model_path)
# print('Saved trained model at %s ' % model_path)

# Score trained model.
# scores = model.evaluate(testImages, testLabels, verbose=1)
# print('Test loss:', scores[0])
# print('Test accuracy:', scores[1])
